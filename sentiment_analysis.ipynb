{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNv9N+OIlNunixSW6aysWJV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gG0XAX_HKOqp"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import re\r\n","import string\r\n","from nltk.corpus import stopwords\r\n","from nltk.tokenize import word_tokenize\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.model_selection import train_test_split\r\n","from nltk.stem import PorterStemmer\r\n","from nltk.stem import WordNetLemmatizer\r\n","# ML Libraries\r\n","from sklearn.metrics import accuracy_score\r\n","from sklearn.naive_bayes import MultinomialNB\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.svm import SVC\r\n","\r\n","# Global Parameters\r\n","stop_words = set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXDC3zmUKW_o"},"source":["def load_dataset(filename, cols):\r\n","    dataset = pd.read_csv(filename, encoding='latin-1')\r\n","    dataset.columns = cols\r\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsRGo9RGKZLc"},"source":["def remove_unwanted_cols(dataset, cols):\r\n","    for col in cols:\r\n","        del dataset[col]\r\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzmc4ov5KbN9"},"source":["def preprocess_tweet_text(tweet):\r\n","    tweet.lower()\r\n","    # Remove urls\r\n","    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\r\n","    # Remove user @ references and '#' from tweet\r\n","    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\r\n","    # Remove punctuations\r\n","    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\r\n","    # Remove stopwords\r\n","    tweet_tokens = word_tokenize(tweet)\r\n","    filtered_words = [w for w in tweet_tokens if not w in stop_words]\r\n","    \r\n","    #ps = PorterStemmer()\r\n","    #stemmed_words = [ps.stem(w) for w in filtered_words]\r\n","    #lemmatizer = WordNetLemmatizer()\r\n","    #lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\r\n","    \r\n","    return \" \".join(filtered_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZ3Gk848Kc6B"},"source":["def get_feature_vector(train_fit):\r\n","    vector = TfidfVectorizer(sublinear_tf=True)\r\n","    vector.fit(train_fit)\r\n","    return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlK6zkr7KeP1"},"source":["def int_to_string(sentiment):\r\n","    if sentiment == 0:\r\n","        return \"Negative\"\r\n","    elif sentiment == 2:\r\n","        return \"Neutral\"\r\n","    else:\r\n","        return \"Positive\"```"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spb0rd0kKgD2"},"source":["# Load dataset\r\n","dataset = load_dataset(\"data/training.csv\", ['target', 't_id', 'created_at', 'query', 'user', 'text'])\r\n","# Remove unwanted columns from dataset\r\n","n_dataset = remove_unwanted_cols(dataset, ['t_id', 'created_at', 'query', 'user'])\r\n","#Preprocess data\r\n","dataset.text = dataset['text'].apply(preprocess_tweet_text)\r\n","# Split dataset into Train, Test\r\n","\r\n","# Same tf vector will be used for Testing sentiments on unseen trending data\r\n","tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\r\n","X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\r\n","y = np.array(dataset.iloc[:, 0]).ravel()\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\r\n","\r\n","# Training Naive Bayes model\r\n","NB_model = MultinomialNB()\r\n","NB_model.fit(X_train, y_train)\r\n","y_predict_nb = NB_model.predict(X_test)\r\n","print(accuracy_score(y_test, y_predict_nb))\r\n","\r\n","# Training Logistics Regression model\r\n","LR_model = LogisticRegression(solver='lbfgs')\r\n","LR_model.fit(X_train, y_train)\r\n","y_predict_lr = LR_model.predict(X_test)\r\n","print(accuracy_score(y_test, y_predict_lr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GF4V9eDoKiCz"},"source":["test_file_name = \"trending_tweets/08-04-2020-1586291553-tweets.csv\"\r\n","test_ds = load_dataset(test_file_name, [\"t_id\", \"hashtag\", \"created_at\", \"user\", \"text\"])\r\n","test_ds = remove_unwanted_cols(test_ds, [\"t_id\", \"created_at\", \"user\"])\r\n","\r\n","# Creating text feature\r\n","test_ds.text = test_ds[\"text\"].apply(preprocess_tweet_text)\r\n","test_feature = tf_vector.transform(np.array(test_ds.iloc[:, 1]).ravel())\r\n","\r\n","# Using Logistic Regression model for prediction\r\n","test_prediction_lr = LR_model.predict(test_feature)\r\n","\r\n","# Averaging out the hashtags result\r\n","test_result_ds = pd.DataFrame({'hashtag': test_ds.hashtag, 'prediction':test_prediction_lr})\r\n","test_result = test_result_ds.groupby(['hashtag']).max().reset_index()\r\n","test_result.columns = ['heashtag', 'predictions']\r\n","test_result.predictions = test_result['predictions'].apply(int_to_string)\r\n","\r\n","print(test_result)"],"execution_count":null,"outputs":[]}]}